{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY0x0BQqU61fpaz1e/2f/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuelajala01/cartpole-v1-gym/blob/main/cartpole_v1_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Af9RWJ3m05",
        "outputId": "936bd351-2637-4445-91b7-66bbc3d03bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "for i in gym.envs.registry.keys():\n",
        " print(i)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeIlnRqo34-r",
        "outputId": "d81b5967-6297-4af0-fffa-26f6e2ce566a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CartPole-v0\n",
            "CartPole-v1\n",
            "MountainCar-v0\n",
            "MountainCarContinuous-v0\n",
            "Pendulum-v1\n",
            "Acrobot-v1\n",
            "phys2d/CartPole-v0\n",
            "phys2d/CartPole-v1\n",
            "phys2d/Pendulum-v0\n",
            "LunarLander-v3\n",
            "LunarLanderContinuous-v3\n",
            "BipedalWalker-v3\n",
            "BipedalWalkerHardcore-v3\n",
            "CarRacing-v3\n",
            "Blackjack-v1\n",
            "FrozenLake-v1\n",
            "FrozenLake8x8-v1\n",
            "CliffWalking-v1\n",
            "CliffWalkingSlippery-v1\n",
            "Taxi-v3\n",
            "tabular/Blackjack-v0\n",
            "tabular/CliffWalking-v0\n",
            "Reacher-v2\n",
            "Reacher-v4\n",
            "Reacher-v5\n",
            "Pusher-v2\n",
            "Pusher-v4\n",
            "Pusher-v5\n",
            "InvertedPendulum-v2\n",
            "InvertedPendulum-v4\n",
            "InvertedPendulum-v5\n",
            "InvertedDoublePendulum-v2\n",
            "InvertedDoublePendulum-v4\n",
            "InvertedDoublePendulum-v5\n",
            "HalfCheetah-v2\n",
            "HalfCheetah-v3\n",
            "HalfCheetah-v4\n",
            "HalfCheetah-v5\n",
            "Hopper-v2\n",
            "Hopper-v3\n",
            "Hopper-v4\n",
            "Hopper-v5\n",
            "Swimmer-v2\n",
            "Swimmer-v3\n",
            "Swimmer-v4\n",
            "Swimmer-v5\n",
            "Walker2d-v2\n",
            "Walker2d-v3\n",
            "Walker2d-v4\n",
            "Walker2d-v5\n",
            "Ant-v2\n",
            "Ant-v3\n",
            "Ant-v4\n",
            "Ant-v5\n",
            "Humanoid-v2\n",
            "Humanoid-v3\n",
            "Humanoid-v4\n",
            "Humanoid-v5\n",
            "HumanoidStandup-v2\n",
            "HumanoidStandup-v4\n",
            "HumanoidStandup-v5\n",
            "GymV21Environment-v0\n",
            "GymV26Environment-v0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "OJ5VOiyR39vI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"observation space: \", env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v__Y4FW44B6U",
        "outputId": "0846a15f-d1bf-464c-8a42-e548e950d88f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation space:  Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation, info = env.reset()\n",
        "print(\"observation: \", observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ojGpiXl4GKU",
        "outputId": "247c9700-67c9-41b7-89d4-b0940844f930"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation:  [ 0.00635547 -0.03516999 -0.00747336  0.04203025]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"action space: \", env.action_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po69ELiK4Lt8",
        "outputId": "57c33f41-7e55-4583-c27c-77e5da009f78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action space:  Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1111\n",
        "env.reset(seed=SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHBW0Iei4VUQ",
        "outputId": "3932f77f-826d-4331-eb02-7d8d1fa20e8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.03593444,  0.01567786, -0.00182151,  0.01612927], dtype=float32),\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch,torch.nn\n",
        "import torch.optim as optim\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EpuWVU834awB",
        "outputId": "2ab52d85-cd02-445e-c1d4-c6bcf1be5578"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e11f4496610>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class PolicyNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer2 = torch.nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "C1chFueK4xdR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_stepwise_returns(rewards, discount_factor):\n",
        "    returns = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + R * discount_factor\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    normalized_returns = (returns - returns.mean()) / returns.std()\n",
        "    return normalized_returns"
      ],
      "metadata": {
        "id": "L4YCZsfR5Q3T"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(stepwise_returns, log_prob_actions):\n",
        "    loss = -(stepwise_returns * log_prob_actions).sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "iQn3tZnG5aDU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_policy(stepwise_returns, log_prob_actions, optimizer):\n",
        "    stepwise_returns = stepwise_returns.detach()\n",
        "    loss = calculate_loss(stepwise_returns, log_prob_actions)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "Cue4DFMN5cgk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    MAX_EPOCHS = 500\n",
        "    DISCOUNT_FACTOR = 0.99\n",
        "    N_TRIALS = 25\n",
        "    REWARD_THRESHOLD = 475\n",
        "    PRINT_INTERVAL = 10\n",
        "    INPUT_DIM = env.observation_space.shape[0]\n",
        "    HIDDEN_DIM = 128\n",
        "    OUTPUT_DIM = env.action_space.n\n",
        "    DROPOUT = 0.5\n",
        "    episode_returns = []\n",
        "    policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
        "    LEARNING_RATE = 0.01\n",
        "    optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)\n",
        "    for episode in range(1, MAX_EPOCHS+1):\n",
        "        episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR)\n",
        "        _ = update_policy(stepwise_returns, log_prob_actions, optimizer)\n",
        "        episode_returns.append(episode_return)\n",
        "        mean_episode_return = np.mean(episode_returns[-N_TRIALS:])\n",
        "        if episode % PRINT_INTERVAL == 0:\n",
        "            print(f'| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |')\n",
        "        if mean_episode_return >= REWARD_THRESHOLD:\n",
        "            print(f'Reached reward threshold in {episode} episodes')\n",
        "            break"
      ],
      "metadata": {
        "id": "vWrlpQIW5ffn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49TP83sB5if0",
        "outputId": "75992def-ecc1-41a1-b3fe-a23d843815b5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Episode:  10 | Mean Rewards:  33.4 |\n",
            "| Episode:  20 | Mean Rewards:  50.2 |\n",
            "| Episode:  30 | Mean Rewards:  57.0 |\n",
            "| Episode:  40 | Mean Rewards:  85.7 |\n",
            "| Episode:  50 | Mean Rewards: 105.9 |\n",
            "| Episode:  60 | Mean Rewards: 131.2 |\n",
            "| Episode:  70 | Mean Rewards: 145.2 |\n",
            "| Episode:  80 | Mean Rewards: 213.2 |\n",
            "| Episode:  90 | Mean Rewards: 193.4 |\n",
            "| Episode: 100 | Mean Rewards: 119.0 |\n",
            "| Episode: 110 | Mean Rewards:  65.2 |\n",
            "| Episode: 120 | Mean Rewards:  95.4 |\n",
            "| Episode: 130 | Mean Rewards: 198.8 |\n",
            "| Episode: 140 | Mean Rewards: 242.8 |\n",
            "| Episode: 150 | Mean Rewards: 195.4 |\n",
            "| Episode: 160 | Mean Rewards: 143.1 |\n",
            "| Episode: 170 | Mean Rewards: 121.1 |\n",
            "| Episode: 180 | Mean Rewards:  92.5 |\n",
            "| Episode: 190 | Mean Rewards:  60.0 |\n",
            "| Episode: 200 | Mean Rewards:  53.7 |\n",
            "| Episode: 210 | Mean Rewards:  75.6 |\n",
            "| Episode: 220 | Mean Rewards: 124.8 |\n",
            "| Episode: 230 | Mean Rewards: 256.1 |\n",
            "| Episode: 240 | Mean Rewards: 397.4 |\n",
            "| Episode: 250 | Mean Rewards: 370.2 |\n",
            "| Episode: 260 | Mean Rewards: 211.2 |\n",
            "| Episode: 270 | Mean Rewards:  90.7 |\n",
            "| Episode: 280 | Mean Rewards:  79.0 |\n",
            "| Episode: 290 | Mean Rewards: 104.6 |\n",
            "| Episode: 300 | Mean Rewards: 136.1 |\n",
            "| Episode: 310 | Mean Rewards: 133.6 |\n",
            "| Episode: 320 | Mean Rewards:  96.2 |\n",
            "| Episode: 330 | Mean Rewards:  66.6 |\n",
            "| Episode: 340 | Mean Rewards:  71.4 |\n",
            "| Episode: 350 | Mean Rewards: 119.3 |\n",
            "| Episode: 360 | Mean Rewards: 287.0 |\n",
            "| Episode: 370 | Mean Rewards: 434.9 |\n",
            "Reached reward threshold in 374 episodes\n"
          ]
        }
      ]
    }
  ]
}