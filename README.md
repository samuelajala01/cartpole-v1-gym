CartPole-v1 — Conceptual Deconstruction

This repository documents my step-by-step understanding of the CartPole-v1
environment and the reinforcement learning principles behind solving it.

## Daily Questions (Populated At Random times)

- :ballot_box_with_check: What is CartPole-v1 actually modelling?
- :ballot_box_with_check: What is the state space, and why are these variables chosen?
- :ballot_box_with_check: What does the action space represent physically?
- :ballot_box_with_check: What is the reward function, and what behavior does it incentivize?
- :ballot_box_with_check: When does an episode terminate, and why?
- :ballot_box_with_check: Why is CartPole considered an “easy” benchmark?
- :ballot_box_with_check: Does the agent receive a reward for each action per step?
- [ ] What policy is implicitly learned in a typical solution?
- [ ] Where exactly does learning happen in the code I copied?
- [ ] Why does random exploration fail beyond a point?
- [ ] Why does CartPole not require long-horizon planning?
- [ ] What assumptions does CartPole make that do NOT hold for real robots?
