CartPole-v1 — Conceptual Deconstruction

This repository documents my step-by-step understanding of the CartPole-v1
environment and the reinforcement learning principles behind solving it.

## Daily Questions (Populated At Random times)

- [ ] What is CartPole-v1 actually modeling?
- [ ] What is the state space, and why are these variables chosen?
- [ ] What does the action space represent physically?
- [ ] What is the reward function, and what behavior does it incentivize?
- [ ] When does an episode terminate, and why?
- [ ] Why is CartPole considered an “easy” benchmark?
- [ ] What policy is implicitly learned in a typical solution?
- [ ] Where exactly does learning happen in the code I copied?
- [ ] Why does random exploration fail beyond a point?
- [ ] Why does CartPole not require long-horizon planning?
- [ ] What assumptions does CartPole make that do NOT hold for real robots?
